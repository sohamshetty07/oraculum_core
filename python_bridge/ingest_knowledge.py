# python_bridge/ingest_knowledge.py
# THE BUILDER: Converts filtered Reddit data into a Searchable Brain (LanceDB)
# Dynamic Data Only - No Hardcoded Facts

import os
import json
import lancedb
from sentence_transformers import SentenceTransformer

# CONFIGURATION
DB_PATH = "./knowledge_db"
# This file must be generated by 'filter_reddit.py' first
FILTERED_DATA_PATH = "./python_bridge/filtered_india_context.jsonl" 
MODEL_NAME = "all-MiniLM-L6-v2"

def build_knowledge_base():
    print(f"üß† KNOWLEDGE BUILDER: Initializing at {DB_PATH}...")
    
    # 1. Check for Dynamic Data Source
    if not os.path.exists(FILTERED_DATA_PATH):
        print(f"‚ùå FATAL ERROR: Data file not found at {FILTERED_DATA_PATH}")
        print("   You must run 'python python_bridge/filter_reddit.py' to generate the dataset first.")
        return

    # 2. Initialize Vector Database & Model
    try:
        db = lancedb.connect(DB_PATH)
        model = SentenceTransformer(MODEL_NAME)
    except Exception as e:
        print(f"‚ùå DB INITIALIZATION FAILED: {e}")
        return

    # 3. Read Data into Memory
    print(f"üìÇ Reading Filtered Data: {FILTERED_DATA_PATH}")
    documents = []
    try:
        with open(FILTERED_DATA_PATH, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line)
                    documents.append({
                        "text": item.get('text', ''),
                        "source": item.get('source', 'reddit_india'),
                        "category": "voice_of_customer",
                        "score": item.get('score', 0),
                        "date": item.get('date', 0)
                    })
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        print(f"‚ùå Error reading file: {e}")
        return

    if not documents:
        print("‚ö†Ô∏è Warning: Data file is empty.")
        return

    print(f"üî¢ Processing {len(documents)} context items...")
    
    # 4. Batch Embed & Ingest (Crucial for performance/RAM)
    batch_size = 500
    table_name = "memory_bank"
    table = None
    
    # If table exists, drop it to ensure clean dynamic data state
    if table_name in db.table_names():
        db.drop_table(table_name)

    total_batches = (len(documents) + batch_size - 1) // batch_size

    for i in range(0, len(documents), batch_size):
        batch = documents[i : i + batch_size]
        
        # Create Embeddings
        texts = [d["text"] for d in batch]
        vectors = model.encode(texts)
        
        # Attach vectors to data objects
        batch_data = []
        for j, doc in enumerate(batch):
            doc["vector"] = vectors[j]
            batch_data.append(doc)
            
        # Write to DB
        if table is None:
            # Create table with first batch
            table = db.create_table(table_name, data=batch_data, mode="overwrite")
        else:
            # Append subsequent batches
            table.add(data=batch_data)
            
        print(f"   ‚îî‚îÄ‚îÄ Embedded Batch {i//batch_size + 1}/{total_batches} ({len(batch)} items)", end='\r')

    print(f"\n‚úÖ SUCCESS: Indian Context Brain fully rebuilt with {len(documents)} records.")

if __name__ == "__main__":
    build_knowledge_base()